import os
import sys
import csv
import time
import zipfile
from google.cloud import storage


from test_bucket import download_files_from_bucket, process_files_to_pdf
from ocr import batch_process_documents, extract_text_from_document
from extracter import extract_document_info



# ─── CONFIGURABLE PATHS TO YOUR STATIC CSVs ───────────────────────────────────
# Place your Elenco Personale.csv and Cluster_Docs.csv under resources/
BASE_DIR = os.path.dirname(__file__)
PERSONNEL_CSV = os.path.join(BASE_DIR, "resources", "Elenco_Personale.csv")
CLUSTER_CSV   = os.path.join(BASE_DIR, "resources", "Cluster_Docs.csv")

# in‐memory maps
cluster_map   = [
    "Provvedimenti a favore",
    "Supervisione Mifid",
    "Flessibilità orarie",
    "Polizza sanitaria",
    "Formazione",
    "Fringe benefits",
    "Assunzione matricola",
    "Primo impiego",
    "Fondo pensione",
    "Destinazione TFR",
    "Nessun cluster",
    "Nomina titolarità",
    "Assegnazione ruolo",
    "Part-time",
    "Cessazione",
    "Proroga TD",
    "Provvedimenti disciplinari",
    "Trasferimento",
    "Lettera assunzione",
    "Trasformazione TI",
    "Proposta di assunzione",
]




def process_document_local(fpdf, ftxt):

    print(f"Processing {fpdf} with the OCR output {ftxt}")

    # Read the text file generated by OCR
    with open(ftxt, "r", encoding="utf-8") as f:
        text = f.read()

    # Pass the text to the extraction function
    extracted = extract_document_info(text, filename=os.path.basename(fpdf))



    return {
        "filename":            filename,
        "person_number":       person_number,
        "document_type":       document_type,
        "country":             country,
        "document_code":       document_code,
        "document_name":       document_name,
        "date_from":           date_from,
        "date_to":             "",              # always empty
        "source_system_owner": "PEOPLE",
        "source_system_id":    document_code,
    }

def generate_documents_of_record_records(docs):
    out = []
    for d in docs:
        out.append("|".join([
            d["filename"],
            "MERGE",
            "DocumentsOfRecord",
            d["person_number"],
            d["document_type"],
            d["country"],
            d["document_code"],
            d["document_name"],
            d["date_from"],
            d["date_to"],
            d["source_system_owner"],
            d["source_system_id"],
        ]))
    return out

def generate_document_attachment_records(docs):
    out = []
    for d in docs:
        out.append("|".join([
            d["filename"],
            "MERGE",
            "DocumentAttachment",
            d["person_number"],
            d["document_type"],
            d["country"],
            d["document_code"],
            "FILE",                 # DataTypeCode
            d["filename"],          # URLorTextorFileName
            d["filename"],          # Title
            d["filename"],          # File
            d["source_system_owner"],
            d["source_system_id"],
        ]))
    return out

def write_dat_file(dor_records, da_records, output_path):
    with open(output_path, "w", encoding="utf-8") as f:
        # Section 1
        f.write(
            "FILENAME|METADATA|DocumentsOfRecord|PersonNumber|DocumentType|Country|"
            "DocumentCode|DocumentName|DateFrom|DateTo|SourceSystemOwner|SourceSystemId\n"
        )
        for line in dor_records:
            f.write(line + "\n")

        # Section 2
        f.write(
            "FILENAME|METADATA|DocumentAttachment|PersonNumber|DocumentType|Country|"
            "DocumentCode|DataTypeCode|URLorTextorFileName|Title|File|"
            "SourceSystemOwner|SourceSystemId\n"
        )
        for line in da_records:
            f.write(line + "\n")

def create_solution_zip(dat_file_path, blob_files_dir, zip_path):
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:
        # add the .dat
        z.write(dat_file_path, arcname=os.path.basename(dat_file_path))
        # add every file under blob_files_dir into BlobFiles/
        for root, _, files in os.walk(blob_files_dir):
            for fname in files:
                full = os.path.join(root, fname)
                arc  = os.path.join("BlobFiles", fname)
                z.write(full, arcname=arc)

def upload_to_gcs(storage_client, bucket_name, run_id, local_file_path):
    bucket = storage_client.bucket(bucket_name)
    dest   = f"{run_id}/solution.zip"
    blob   = bucket.blob(dest)
    blob.upload_from_filename(local_file_path)
    print(f"Caricamento completato: gs://{bucket_name}/{dest}")

def main():
    # 1) Read env vars (or fall back for local testing)
    run_id       = os.environ.get("RUN_ID")
    input_bucket = os.environ.get("INPUT_BUCKET")
    output_bucket= os.environ.get("OUTPUT_BUCKET")
    if not (run_id and input_bucket and output_bucket):
        run_id        = str(int(time.time()))
        input_bucket  = "credemhack-documents-iam"
        output_bucket = "credemhack-output-iam"

    # 2) Init GCS client
    storage_client = storage.Client()

    # 3) Prepare local dirs
    out_base = "output"
    os.makedirs(out_base, exist_ok=True)
    dat_path = os.path.join(out_base, "DocumentsOfRecord.dat")
    blob_dir = os.path.join(out_base, "BlobFiles")
    os.makedirs(blob_dir, exist_ok=True)

    # 4) Download & convert to PDF
    download_files_from_bucket(input_bucket, blob_dir)
    pdf_dir = os.path.join(out_base, "pdf_files")
    process_files_to_pdf(blob_dir, pdf_dir)

    # 5) OCR‐batch (if you need it elsewhere)
    ocr_output = os.path.join(out_base, "ocr_output")
    batch_process_documents(
        project_id   = "credemhack-iam",
        location     = "us",
        processor_id = "906fe5719131d935",
        input_folder = pdf_dir,
        output_folder= ocr_output
    )

    # 7) Process each PDF
    processed = []
    for fname in os.listdir(pdf_dir):
        if not fname.lower().endswith(".pdf"):
            continue
        fpdf = os.path.join(pdf_dir, fname)
        ftxt = os.path.join(ocr_output, fname.replace(".pdf", ".txt"))
        rec = process_document_local(fpdf, ftxt)
        processed.append(rec)

    # 8) Build records
    dor = generate_documents_of_record_records(processed)
    da  = generate_document_attachment_records(processed)

    # 9) Write the .dat
    write_dat_file(dor, da, dat_path)

    # 10) Zip up
    zip_path = os.path.join(out_base, "solution.zip")
    create_solution_zip(dat_path, blob_dir, zip_path)

    # 11) Upload
    upload_to_gcs(storage_client, output_bucket, run_id, zip_path)

if __name__ == "__main__":
    main()